<!doctype html>
<html lang="en"> <!-- åˆå§‹åŒ–åä¼šéšæœ¬åœ°å­˜å‚¨åˆ‡æ¢ -->
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>People See Text, But LLM Not | CSU-JPG Lab Stories</title>
    <meta name="description" content="People see text visually; LLMs tokenize. Why this gap matters, what visual tokens change, and where vision-centric MLLMs are heading." />
    <meta property="og:title" content="People See Text, But LLM Not" />
    <meta property="og:description" content="People read visually, not symbolically. Visual tokens and vision-centric MLLMs point to the next paradigm." />
    <meta property="og:type" content="article" />
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='0.9em' font-size='90'>ğŸ§ </text></svg>">
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
      html{scroll-behavior:smooth}
      .prose p{line-height:1.8}
      .yt-wrap{position:relative;padding-top:56.25%}
      .yt-wrap iframe{position:absolute;inset:0;width:100%;height:100%;border:0;border-radius:12px}
      figure img{background:#fff}
      .lead{color:#525252}

      /* === Markdown-like (GitHub-ish) reset for prose === */
      .prose {
        --tw-prose-body: #1f2937;       /* text-gray-800 */
        --tw-prose-headings: #111827;   /* text-gray-900 */
        --tw-prose-links: #2563eb;      /* blue-600 */
        --tw-prose-bold: #111827;       /* darker bold */
        --tw-prose-quotes: #374151;     /* gray-700 */
        --tw-prose-counters: #6b7280;   /* gray-500 */
        --tw-prose-bullets: #6b7280;    /* gray-500 */
        --tw-prose-hr: #e5e7eb;         /* gray-200 */
        --tw-prose-code: #111827;       /* text for code */
        --tw-prose-th-borders: #e5e7eb;
        --tw-prose-td-borders: #e5e7eb;
      }
      /* æ ‡é¢˜æ›´â€œç²—â€ï¼Œä¸æ”¹å˜å¤§å°å†™ */
      .prose h1, .prose h2, .prose h3,
      .prose h4, .prose h5, .prose h6 {
        text-transform: none;
        font-weight: 800;
        line-height: 1.25;
      }
      /* H1/H2 è§†è§‰èŠ‚å¥ï¼šH2 å¸¦ä¸‹åˆ’çº¿ */
      .prose h1 { margin-top: 0.6em; margin-bottom: 0.3em; }
      .prose h2 {
        margin-top: 2.2em; margin-bottom: 0.8em;
        padding-bottom: .3rem; border-bottom: 1px solid #e5e7eb;
      }
      .prose h3 { margin-top: 1.6em; margin-bottom: .6em; }

      /* é“¾æ¥æ›´â€œè“â€ï¼Œæ‚¬åœä¸‹åˆ’çº¿ */
      .prose a { color: #2563eb; text-decoration: none; }
      .prose a:hover { text-decoration: underline; color: #1d4ed8; }

      /* å¼•ç”¨å—ï¼šå·¦çº¿ + æµ…åº• + åœ†è§’ */
      .prose blockquote {
        border-left: 4px solid #e5e7eb;
        background: #f9fafb;
        color: #4b5563;
        padding: .75rem 1rem;
        border-radius: .5rem;
        margin: 1rem 0;
        font-style: normal;
      }

      /* ç²—ä½“æ›´é»‘ä¸€äº› */
      .prose strong { font-weight: 800; color: #111827; }

      /* è¡Œå†…ä»£ç ï¼šç°åº•+åœ†è§’ */
      .prose :not(pre) > code {
        background: #f3f4f6;
        padding: .15rem .35rem;
        border-radius: .25rem;
        font-weight: 500;
      }

      /* ä»£ç å—ï¼ˆè‹¥æ–‡ç« åç»­æ·»åŠ ä»£ç ï¼‰ */
      .prose pre {
        background: #0b1220;
        color: #e5e7eb;
        border-radius: .6rem;
        padding: 1rem;
        border: 1px solid #111827;
        overflow: auto;
      }

      /* åˆ—è¡¨é¡¹ç›®ç¬¦å·é¢œè‰²æ›´æŸ” */
      .prose ul > li::marker,
      .prose ol > li::marker { color: #6b7280; }

      /* å›¾ç‰‡ï¼šåœ†è§’+è¾¹æ¡†ï¼Œåƒ MD é¢„è§ˆ */
      .prose img {
        border-radius: .75rem;
        border: 1px solid #e5e7eb;
        background: #fff;
      }

      /* å›¾æ³¨æ›´æ·¡ */
      .prose figure figcaption {
        color: #6b7280;
        font-size: .78rem;
      }

      /* ç›®å½•æ ‡é¢˜èµ°åŒä¸€é£æ ¼ */
      #toc .font-semibold { color:#111827; }
    </style>
  </head>
  <body class="bg-white text-neutral-900">
    <a id="top"></a>
    <!-- ===== Headerï¼ˆä»…ä¸€ä»½ï¼‰ ===== -->
    <header class="border-b border-neutral-200">
      <div class="max-w-4xl mx-auto px-4 py-4 flex items-center justify-between">
        <a id="homeLink" href="../" class="text-sm text-blue-700 hover:underline">â† Home</a>
        <div class="flex items-center gap-3">
          <div id="pubdate" class="text-sm text-neutral-500">Published on 2025-10-21 Â· CSU-JPG Lab</div>
          <div class="h-5 w-px bg-neutral-200"></div>
          <div class="text-sm">
            <button id="btnEN" class="px-2 py-1 rounded bg-black text-white">EN</button>
            <span class="mx-1 text-neutral-300">|</span>
            <button id="btnZH" class="px-2 py-1 rounded hover:bg-neutral-100">ä¸­æ–‡</button>
          </div>
        </div>
      </div>
    </header>

    <main class="max-w-4xl mx-auto px-4 py-10">
      <!-- ===== ç›®å½•ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰ ===== -->
      <nav id="toc" class="mt-2 text-sm"></nav>

      <!-- ================= EN CONTENT ================= -->
      <article id="content-en" data-lang="en" class="mt-6 prose prose-neutral md:prose-lg max-w-none">
        <h1 class="!mb-2">People See Text, But LLM Not</h1>
        <blockquote class="lead">
          <i>â€œAoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn't mttaer in waht oredr the ltteers in a wrod are.â€</i> â€” Davis, Matt (2012)
        </blockquote>
        <p>
          You can probably read that sentence effortlessly. Despite the chaotic order of letters, your brain automatically reconstructs the intended words â€” because humans donâ€™t read text letter by letter. We perceive shapes, patterns, and visual words.
        </p>

        <h2>1. See Text vs. LLM Process Text</h2>

        <h3>1.1 The Visual Nature of Reading</h3>
        <p>
          Cognitive neuroscience has long shown that our brain recruits a specialized region called the <em>Visual Word Form Area</em> (VWFA) in the left occipitotemporal cortex.
          This area recognizes entire word forms as visual objects, not symbolic sequences. Thatâ€™s why humans can read â€œCmabrigdeâ€ as Cambridge, or identify words in distorted fonts, mixed scripts, and complex layouts.
        </p>

                <!-- (1) human_brain.png -->
                <figure class="mt-4">
                  <img src="https://csu-jpg.github.io/Blog/figures/human_brain.png" alt="Visual Word Form Area: reading is visual">
                  <figcaption class="mt-2 text-xs text-neutral-500">"The Science Behind LearningRx Reading Programs". </figcaption>
                </figure>
                The brain â€œseesâ€ words: reading is vision, not just language processing.
        <p><strong>In essence, people see text.</strong> Reading is vision â€” not just language processing.</p>

        <h3>1.2 How LLMs Process Text (and Why Itâ€™s Different)</h3>
        <p>
          Large Language Models (LLMs), in contrast, do not â€œseeâ€ text. They tokenize it â€” breaking sentences into subword units like â€œvisâ€, â€œionâ€, â€œcentâ€, â€œricâ€.
          Each token becomes an integer ID looked up in a vocabulary. This is efficient for symbolic computation, but it destroys the visual and structural continuity of language.
          As a result, the holistic form of text is lost in translation. Humans can easily read â€œt3xtâ€ as â€œtext,â€ but token-based models treat them as unrelated sequences.
        </p>

        <h3>1.3 The Consequences of Tokenization</h3>
        <ul>
          <li><strong>Loss of visual semantics:</strong> Fonts, shapes, and layout cues disappear.</li>
          <li><strong>Over-segmentation in multilingual text:</strong> Low-resource languages get fragmented into meaningless subwords.</li>
          <li><strong>Inefficiency for long text:</strong> A few characters can turn into multiple tokens, inflating context length and cost.</li>
        </ul>

        <b>It is hard to process Interleaved Document  with tokenization: </b>

        <!-- (2) dvq.png (æ”¾åœ¨åˆ†è¯åæœç¤ºæ„) -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/dvq.png" alt="Discrete tokenization vs. visual perception">
          <figcaption class="mt-2 text-xs text-neutral-500">Leonardo da Vinciâ€™s Manuscripts</figcaption>
        </figure>

        <p>
          This is why a model might read a paper or screenshot yet miss equations, tables, or captions â€” because they are treated as pixels, not text.
        </p>

        <h3>1.4 Text Widely Exists in Web Images</h3>
        <p>
          <strong>45%+</strong> web images contain text (e.g., in LAION-2B; Lin et al., Parrot, ECCV 2024). Documents, UI, charts, and designs are inherently <em>visual text</em>.
        </p>
      <!-- (3) parrot.png -->
      <figure class="mt-6">
        <img src="https://csu-jpg.github.io/Blog/figures/parrot.png" alt="Text prevalence in web images (Parrot)">
        <figcaption class="mt-2 text-xs text-neutral-500">Text is ubiquitous in web images: documents, UI, charts, designs.</figcaption>
      </figure>



        <h2>2. Early Attempts: Making Models See Text / Unified Model</h2>
        <p>Several early studies tried to bridge this gap by treating text as an image signal:</p>
        <ul>
          <li><strong>Visual Text Representations</strong> (Salesky et al., EMNLP 2021): The paper introduces visual text representations that replace discrete subword vocabularies with continuous embeddings derived from rendered text. This method matches traditional models in translation quality while being far more robust to noisy or corrupted input.</li>
          <!-- (4) visual_text_representations.png -->
          <figure class="mt-4">
            <img src="https://csu-jpg.github.io/Blog/figures/visual_text_repre.png" alt="visual text representations">
            <figcaption class="mt-2 text-xs text-neutral-500">visual text representations</figcaption>
          </figure>

          <li><strong>PIXEL</strong> (Phillip et al., ICLR 2023): render text as images, pretrain ViT-MAE to reconstruct masked pixels, removing the tokenizer; robust to unseen scripts and orthographic perturbations.</li>
               <!-- (4) pixel.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/pixel.png" alt="PIXEL: render text as images, ViT-MAE pretraining">
          <figcaption class="mt-2 text-xs text-neutral-500">PIXEL (ICLRâ€™23): render text as images and pretrain with MAE.</figcaption>
        </figure>
          <li><strong>CLIPPO</strong> (Michael et al., CVPR 2023): unify image &amp; text under a single pixel-based encoder; text is rendered to images and trained with contrastive loss.</li>
          <!-- (5) clippo.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/clippo.png" alt="CLIPPO pixel-only encoder">
          <figcaption class="mt-2 text-xs text-neutral-500">CLIPPO (CVPRâ€™23): a pixel-only encoder for image and text.</figcaption>
        </figure>
          <li><strong>Pix2Struct</strong> (Lee et al., ICML 2023): parse screenshots as structured visual input for document/layout understanding.</li>
            <!-- (6) pix2struct.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/pix2struct.png" alt="Pix2Struct for screenshot and layout understanding">
          <figcaption class="mt-2 text-xs text-neutral-500">Pix2Struct (ICMLâ€™23): parse screenshots as structured visual input.</figcaption>
        </figure>
        <li><strong>PIXAR</strong>(Tai et al., ACL 2024): PIXAR is pixel-based autoregressive LLM capable of generating readable text, using adversarial pretraining to overcome MLE limitations and reach GPT-2-level performance.
          .</li>
        <!-- (8) pixar.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/pixar.png" alt="PIXAR: a pre-training objective that leverages both visual and textual information to enhance model understanding.">
          <figcaption class="mt-2 text-xs text-neutral-500">PIXAR (ACLâ€™24): PIXAR is pixel-based autoregressive LLM capable of generating readable text, using adversarial pretraining to overcome MLE limitations and reach GPT-2-level performance.</figcaption>
        </figure>
          <li><strong>PTP</strong> (Gao et al., arXiv 2024): screenshot language models with a Patch-and-Text Prediction objective to reconstruct masked image patches and text jointly.</li>
          <!-- (7) ptp.png -->
          <figure class="mt-4">
            <img src="https://csu-jpg.github.io/Blog/figures/ptp.png" alt="PTP screenshot language models">
            <figcaption class="mt-2 text-xs text-neutral-500">PTP (arXivâ€™24): joint patch-and-text prediction on screenshots.</figcaption>
          </figure>

          <!-- (6-2) fuyu.png -->
         <b>Fuyu:</b>precess visual and textual information in a unified model.
          <figure class="mt-4">
            <img src="https://csu-jpg.github.io/Blog/figures/fuyu.png" alt="Fuyu: precess visual and textual information in a unified model.">
            <figcaption class="mt-2 text-xs text-neutral-500">Fuyu (arXiv 24'1): precess visual and textual information in a unified model.</figcaption>
          </figure>

          <li><strong>PEAP</strong> (Lyu et al., arXiv 2025'): a unified perception paradigm for agentic language models that interact directly with real-world environments combining visual and textual information.</li>
          <!-- (7) ptp.png -->
          <figure class="mt-4">
            <img src="https://csu-jpg.github.io/Blog/figures/peap.png" alt="PEAP: a unified perception paradigm for agentic language models that interact directly with real-world environments combining visual and textual information.">
            <figcaption class="mt-2 text-xs text-neutral-500">PEAP (arXivâ€™25): a unified perception paradigm for agentic language models that interact directly with real-world environments combining visual and textual information.</figcaption>
          </figure>
        </ul>
        <p>
          
          These works helped models <em>see</em> text, but left a <span style="color:#d32f2f; font-weight:bold;"><u>key questionï¼š what tangible benefit does transforming text into images actually provide?</u></span>
        </p>

        <h2>3. Recent Attempts: Visual Tokens for Long-Context Compression</h2>
        <p><strong>Key observations:</strong></p>
        <ul>
          <li>1. Vision encoders are typically much smaller than LLMs (e.g., ~100M for ViT-B vs. 7B+ for LLaMA/Mistral).</li>
          <li>2. CLIP-style pretraining yields emergent OCR-like abilities without explicit supervision.</li>
          <li>3. Visual patches can encode dense textual content (more characters per spatial area), effectively extending context via <em>spatial compression</em>.</li>
        </ul>
        </br>
        <p>
          <span style="color:#d32f2f; font-weight:bold;">
            Interleaved document-level multimodal pretraining is an ideal setup.
          </span>
          <br><strong>NeurIPS 2024 â€” â€œLeveraging Visual Tokens for Extended Text Contextsâ€:</strong> represent long text as compact visual tokens, enabling longer &amp; denser context at training and inference.
          This improves in-context text length from <strong>256 â†’ 2048</strong> during pretraining on NVIDIA H100.
        </p>
            <!-- (9) vis_in_context.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vis_in_context.png" alt="Visual tokens for extended in-context learning">
          <figcaption class="mt-2 text-xs text-neutral-500">Leveraging visual tokens for extended in-context understanding.</figcaption>
        </figure>
                <!-- (10) vis_in_context_h100.png -->
                <figure class="mt-4">
                  <img src="https://csu-jpg.github.io/Blog/figures/visual_in_context_h100.png" alt="H100 pretraining: text context 256â†’2048">
                  <figcaption class="mt-2 text-xs text-neutral-500">On NVIDIA H100: in-context text length improved from 256 â†’ 2048.</figcaption>
                </figure>
            </br>
        <p>
          <span style="color:#d32f2f; font-weight:bold;">
            Long-text understanding in LLM is another ideal situation.
          </span>
          <br><strong>Xing et al., NeurIPS 2025 â€” â€œVision-Centric Token Compression in Large Language Modelsâ€ (VIST):</strong>
          inspired by a human slowâ€“fast reading circuit: a <em>fast</em> visualized path renders distant context as images for quick semantic extraction; a <em>slow</em> path feeds key text to LLM for deep reasoning.
        </p>

        <!-- (11) vist.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vist.png" alt="VIST: vision-centric token compression in LLMs">
          <figcaption class="mt-2 text-xs text-neutral-500">VIST (NeurIPSâ€™25): fast visual path + slow language path.</figcaption>
        </figure>
        </br>


        <span style="color:#d32f2f; font-weight:bold;">
          Deepseek's answer: OCR.
        </span>
        <p>
          <strong>DeepSeek-OCR (Oct 2025): Contextual Optical Compression</strong> extends visual-token compression to OCR:
          compressing thousands of text tokens into a few hundred visual reps, reaching <strong>~97% OCR precision at ~10Ã— compression</strong>.
          DeepSeek-OCR was driven by <strong>powerful infrastructure</strong> and 
          <strong>painstaking large-scale data preparation</strong>, enabling the model to scale visualâ€“token compression far beyond prior works.
        </p>
        <!-- (12) deepseek_motivation.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/deepseek_motivation.png" alt="DeepSeek-OCR contextual optical compression">
          <figcaption class="mt-2 text-xs text-neutral-500">DeepSeek-OCR: ~97% precision with ~10Ã— compression.</figcaption>
        </figure>
        <p><em>The convergence of visual perception and language understanding is not a coincidence â€” it is the next paradigm shift.</em></p>


        <h2>4. The Future Ahead: A Vision-centric MLLM</h2>
      <p>
        In a truly vision-centric multimodal language model, we may no longer need a traditional tokenizer.
        The model could read text visually â€” as humans do â€” and even generate text as images, <b> <u>unifying perception and generation in the same visual space.</u> </b>
      </p>
      <span style="color:#d32f2f; font-weight:bold;"><u><b> Dense Text Image Generation:</b></u></span>
      <p>
        To reach that goal, we must perfect image-based text rendering and long-text visual generation:
        <strong>TextAtlas5M</strong> provides large-scale dense text rendering where captions, documents, and designs are visually represented.
            <!-- (13) textatlas_5m.png -->
      <figure class="mt-4">
        <img src="https://csu-jpg.github.io/Blog/figures/text_atlas_5m.png" alt="TextAtlas5M dataset for dense text rendering">
        <figcaption class="mt-2 text-xs text-neutral-500">TextAtlas5M: large-scale dense text rendering. Arxiv 25'2.</figcaption>
      </figure>
        <br><em>Beyond Words</em> aims to generate text-heavy, information-dense images from natural prompts, pushing multimodal autoregressive models toward true long-text visual generation.
         <!-- (14) long_text_ar.png -->
      <figure class="mt-4">
        <img src="https://csu-jpg.github.io/Blog/figures/long_text_ar.png" alt="Beyond Words: long-text visual generation">
        <figcaption class="mt-2 text-xs text-neutral-500"><em>Beyond Words</em>: toward long-text visual generation. Arxiv 25'2.</figcaption>
      </figure>
      </p>

      <span style="color:#d32f2f; font-weight:bold;"><u><b> More tasks in LLM:</b></u></span>
      <p>
        Xing et al, Arxiv25'10 - "SEE THE TEXT: FROM TOKENIZATION TO VISUAL READING", this work extend this idea to more tasks include Classification, and QA.
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/see_the_text.png" alt="See The Text">
          <figcaption class="mt-2 text-xs text-neutral-500">See The Text (Arxiv25'10)</figcaption>
        </figure>
      </p>
      
        <h2>5. Toward the Next Generation of Vision-centric MLLM</h2>
        <ul>
          <li>Text is <strong>visual</strong>, not merely symbolic.</li>
          <li>Vision is <strong>language</strong>, not separate.</li>
          <li>Compression is <strong>perception</strong>, not just engineering.</li>
        </ul>
        <p>
          The ultimate goal is a model that reads, writes, and sees text the way humans do â€” through the eyes of vision.
          <br/><strong>People see text. Soon, LLMs &amp; LVMs will too.</strong>
        </p>
        <!-- <p>
          We compiled a visual text paper list: See 
          <a href="https://github.com/CSU-JPG/Awesome-Visual-Text" target="_blank" rel="noopener noreferrer">https://github.com/CSU-JPG/Awesome-Visual-Text</a>
          for details.
        </p> -->
      </article>

      <!-- ================= ZH CONTENT ================= -->
      <article id="content-zh" data-lang="zh" class="mt-6 prose prose-neutral md:prose-lg max-w-none" hidden>
        <h1 class="!mb-2">äººèƒ½â€œçœ‹è§â€æ–‡å­—ï¼Œä½† LLM å´ä¸èƒ½</h1>
        <blockquote class="lead">
          â€œAoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn't mttaer in waht oredr the ltteers in a wrod are.â€ â€” Davis, Matt (2012)
        </blockquote>
        <p>
          ä½ å¤§æ¦‚ç‡å¯ä»¥æ¯«ä¸è´¹åŠ›åœ°è¯»æ‡‚è¿™å¥è¯ã€‚è™½ç„¶å­—æ¯é¡ºåºè¢«æ‰“ä¹±ï¼Œä½ çš„å¤§è„‘ä»èƒ½è‡ªåŠ¨å¤åŸâ€”â€”å› ä¸ºäººç±»é˜…è¯»å¹¶ä¸æ˜¯é€å­—æ¯è§£ç ï¼Œè€Œæ˜¯åœ¨çœ‹è¯å½¢ã€ç»“æ„ä¸è§†è§‰æ¨¡å¼ã€‚
        </p>

        <h2>1. äººâ€œçœ‹è§â€æ–‡æœ¬ vs. LLMâ€œå¤„ç†â€æ–‡æœ¬</h2>

        <h3>1.1 é˜…è¯»çš„è§†è§‰æœ¬è´¨</h3>
        <p>
          è®¤çŸ¥ç¥ç»ç§‘å­¦è¡¨æ˜ï¼šå¤§è„‘åœ¨æ•é¢å¶å·¦ä¾§çš„ä¸€ä¸ªä¸“é—¨åŒºåŸŸâ€”â€”<em>è§†è§‰è¯å½¢åŒº</em>ï¼ˆVWFAï¼‰å‚ä¸é˜…è¯»ã€‚
          å®ƒæŠŠæ•´ä¸ªè¯å½“ä½œè§†è§‰å¯¹è±¡æ¥è¯†åˆ«ï¼Œè€Œéå•çº¯çš„ç¬¦å·åºåˆ—ã€‚å› æ­¤æˆ‘ä»¬èƒ½æŠŠâ€œCmabrigdeâ€è¯»æˆ Cambridgeï¼Œä¹Ÿèƒ½åœ¨æ‰­æ›²å­—ä½“ã€æ··åˆä¹¦å†™ç³»ç»Ÿã€å¤æ‚ç‰ˆå¼ä¸­è¯†åˆ«æ–‡å­—ã€‚
        </p>

          <!-- (1) human_brain.png -->
          <figure class="mt-4">
            <img src="https://csu-jpg.github.io/Blog/figures/human_brain.png" alt="Visual Word Form Area: reading is visual">
            <figcaption class="mt-2 text-xs text-neutral-500">"The Science Behind LearningRx Reading Programs". </figcaption>
          </figure>

        <p><strong>æœ¬è´¨ä¸Šï¼Œäººç±»æ˜¯åœ¨â€œçœ‹â€æ–‡æœ¬ã€‚</strong> é˜…è¯»é¦–å…ˆæ˜¯è§†è§‰ï¼Œè€Œä¸ä»…æ˜¯è¯­è¨€ã€‚</p>

        <h3>1.2 LLM å¦‚ä½•å¤„ç†æ–‡æœ¬ï¼ˆä»¥åŠå·®å¼‚ä½•åœ¨ï¼‰</h3>
        <p>
          ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§è¯­è¨€æ¨¡å‹å¹¶ä¸â€œçœ‹è§â€æ–‡æœ¬ã€‚å®ƒä»¬å…ˆå°†æ–‡æœ¬ <em>åˆ†è¯</em> â€”â€”æŠŠå¥å­æ‰“æˆâ€œvis / ion / cent / ricâ€ç­‰å­è¯å•å…ƒï¼›æ¯ä¸ª token æ˜ å°„ä¸ºè¯è¡¨ä¸­çš„ä¸€ä¸ªæ•´æ•° IDã€‚
          è¿™å¯¹ç¬¦å·è®¡ç®—å¾ˆé«˜æ•ˆï¼Œä½†ä¼šç ´åè¯­è¨€çš„è§†è§‰ä¸ç»“æ„è¿ç»­æ€§ã€‚
          ç»“æœæ˜¯ï¼Œæ–‡æœ¬çš„æ•´ä½“å½¢æ€åœ¨æ˜ å°„ä¸­ä¸¢å¤±ã€‚äººç±»èƒ½æŠŠ â€œt3xtâ€ è¯»æˆ â€œtextâ€ï¼Œä½†åŸºäº token çš„æ¨¡å‹ä¼šå°†å®ƒä»¬å½“ä½œæ— å…³åºåˆ—ã€‚
        </p>

        <h3>1.3 åˆ†è¯å¸¦æ¥çš„åæœ</h3>
        <ul>
          <li><strong>è§†è§‰è¯­ä¹‰ä¸¢å¤±ï¼š</strong>å­—ä½“ã€å½¢çŠ¶ä¸ç‰ˆå¼çº¿ç´¢æ¶ˆå¤±ã€‚</li>
          <li><strong>å¤šè¯­æ–‡æœ¬è¿‡åº¦åˆ‡åˆ†ï¼š</strong>èµ„æºç¨€ç¼ºè¯­è¨€è¢«æ‹†æˆç¼ºä¹è¯­ä¹‰çš„ç‰‡æ®µã€‚</li>
          <li><strong>é•¿æ–‡æœ¬ä½æ•ˆï¼š</strong>å°‘é‡å­—ç¬¦å¯èƒ½å¯¹åº”å¤šä¸ª tokenï¼Œæ”¾å¤§ä¸Šä¸‹æ–‡é•¿åº¦ä¸æˆæœ¬ã€‚</li>
        </ul>

        <!-- (2) dvq.png (æ”¾åœ¨åˆ†è¯åæœç¤ºæ„) -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/dvq.png" alt="Discrete tokenization vs. visual perception">
          <figcaption class="mt-2 text-xs text-neutral-500">è¾¾Â·èŠ¬å¥‡æ‰‹ç¨¿</figcaption>
        </figure>

        <p>
          è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ¨¡å‹â€œçœ‹è¿‡â€è®ºæ–‡æˆ–æˆªå›¾ï¼Œå´å®¹æ˜“æ¼æ‰å…¬å¼ã€è¡¨æ ¼æˆ–å›¾æ³¨â€”â€”å› ä¸ºå®ƒä»¬è¢«å½“æˆåƒç´ è€Œéæ–‡æœ¬ã€‚
        </p>

        <h3>1.4 äº’è”ç½‘ä¸Šçš„å›¾åƒä¸­å¹¿æ³›å­˜åœ¨æ–‡æœ¬</h3>
        <p>
          åœ¨å¦‚ LAION-2B ç­‰æ•°æ®ä¸­ï¼Œ<strong>45%+</strong> çš„ç½‘é¡µå›¾åƒåŒ…å«æ–‡æœ¬ï¼ˆLin ç­‰ï¼ŒParrotï¼ŒECCV 2024ï¼‰ã€‚æ–‡æ¡£ã€UIã€å›¾è¡¨ã€è®¾è®¡ï¼Œæœ¬è´¨ä¸Šéƒ½æ˜¯â€œè§†è§‰åŒ–æ–‡æœ¬â€ã€‚
        </p>

              <!-- (3) parrot.png -->
      <figure class="mt-6">
        <img src="https://csu-jpg.github.io/Blog/figures/parrot.png" alt="Text prevalence in web images (Parrot)">
        <figcaption class="mt-2 text-xs text-neutral-500">æ–‡æœ¬å¹¿æ³›å­˜åœ¨äºç½‘é¡µå›¾åƒä¸­ï¼šæ–‡æ¡£ã€UIã€å›¾è¡¨ã€è®¾è®¡ã€‚</figcaption>
      </figure>


        <h2>2. æ—©æœŸå°è¯•ï¼šè®©æ¨¡å‹â€œçœ‹è§â€æ–‡æœ¬</h2>
        <p>ä¸€äº›å·¥ä½œæŠŠæ–‡æœ¬å½“ä½œå›¾åƒä¿¡å·æ¥å»ºæ¨¡ï¼Œä»¥å¼¥åˆè¿™ç§å·®è·ï¼š</p>
        <ul>
          <li><strong>visual text representations</strong>ï¼ˆSalesky ç­‰ï¼ŒEMNLP 2021ï¼‰ï¼šæŠŠæ–‡æœ¬æ¸²æŸ“ä¸ºå›¾åƒï¼Œç”¨ ViT-MAE è¿›è¡Œåƒç´ é‡å»ºé¢„è®­ç»ƒï¼Œå»æ‰ tokenizerï¼›å¯¹æœªè§ä¹¦å†™ç³»ç»Ÿä¸æ­£å­—æ³•æ‰°åŠ¨æ›´é²æ£’ã€‚</li>
          <!-- (4) visual_text_representations.png -->
          <figure class="mt-4">
            <img src="https://csu-jpg.github.io/Blog/figures/visual_text_repre.png" alt="visual text representations">
            <figcaption class="mt-2 text-xs text-neutral-500">visual text representations</figcaption>
          </figure>
          <li><strong>PIXEL</strong>ï¼ˆPhillip ç­‰ï¼ŒICLR 2023ï¼‰ï¼šæŠŠæ–‡æœ¬æ¸²æŸ“ä¸ºå›¾åƒï¼Œç”¨ ViT-MAE è¿›è¡Œåƒç´ é‡å»ºé¢„è®­ç»ƒï¼Œå»æ‰ tokenizerï¼›å¯¹æœªè§ä¹¦å†™ç³»ç»Ÿä¸æ­£å­—æ³•æ‰°åŠ¨æ›´é²æ£’ã€‚</li>
               <!-- (4) pixel.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/pixel.png" alt="PIXEL: render text as images, ViT-MAE pretraining">
          <figcaption class="mt-2 text-xs text-neutral-500">PIXEL (ICLR'23): æŠŠæ–‡æœ¬æ¸²æŸ“ä¸ºå›¾åƒå¹¶ç”¨ MAE é¢„è®­ç»ƒã€‚</figcaption>
        </figure>
          <li><strong>CLIPPO</strong>ï¼ˆMichael ç­‰ï¼ŒCVPR 2023ï¼‰ï¼šç”¨å•ä¸€åƒç´ ç¼–ç å™¨ç»Ÿä¸€å›¾åƒä¸æ–‡æœ¬ï¼›æ–‡æœ¬æ¸²æŸ“ä¸ºå›¾åƒï¼Œé‡‡ç”¨å¯¹æ¯”å­¦ä¹ ã€‚</li>
          <!-- (5) clippo.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/clippo.png" alt="CLIPPO pixel-only encoder">
          <figcaption class="mt-2 text-xs text-neutral-500">CLIPPO (CVPR'23): ç”¨äºå›¾åƒå’Œæ–‡æœ¬çš„çº¯åƒç´ ç¼–ç å™¨ã€‚</figcaption>
        </figure>
          <li><strong>Pix2Struct</strong>ï¼ˆLee ç­‰ï¼ŒICML 2023ï¼‰ï¼šæŠŠæˆªå›¾è§£æä¸ºç»“æ„åŒ–è§†è§‰è¾“å…¥ï¼Œé¢å‘æ–‡æ¡£/ç‰ˆå¼ç†è§£ã€‚</li>
            <!-- (6) pix2struct.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/pix2struct.png" alt="Pix2Struct for screenshot and layout understanding">
          <figcaption class="mt-2 text-xs text-neutral-500">Pix2Struct (ICML'23): æŠŠæˆªå›¾è§£æä¸ºç»“æ„åŒ–è§†è§‰è¾“å…¥ã€‚</figcaption>
        </figure>
        <li><strong>PIXAR</strong>(Tai et al., ACL 2024): åŸºäºåƒç´ çš„è‡ªå›å½’LLMï¼Œèƒ½å¤Ÿç”Ÿæˆå¯è¯»æ–‡æœ¬ï¼Œä½¿ç”¨å¯¹æŠ—æ€§é¢„è®­ç»ƒå…‹æœMLEé™åˆ¶ï¼Œè¾¾åˆ°GPT-2çº§åˆ«æ€§èƒ½ã€‚</li>
        <!-- (8) pixar.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/pixar.png" alt="PIXAR: a pre-training objective that leverages both visual and textual information to enhance model understanding.">
          <figcaption class="mt-2 text-xs text-neutral-500">PIXAR (ACLâ€™24): åŸºäºåƒç´ çš„è‡ªå›å½’LLMï¼Œèƒ½å¤Ÿç”Ÿæˆå¯è¯»æ–‡æœ¬ï¼Œä½¿ç”¨å¯¹æŠ—æ€§é¢„è®­ç»ƒå…‹æœMLEé™åˆ¶ï¼Œè¾¾åˆ°GPT-2çº§åˆ«æ€§èƒ½ã€‚</figcaption>
        </figure>
          <li><strong>PTP</strong>ï¼ˆGao ç­‰ï¼ŒarXiv 2024ï¼‰ï¼šæå‡ºæˆªå›¾è¯­è¨€æ¨¡å‹ä¸ Patch-and-Text Prediction ç›®æ ‡ï¼ŒåŒæ—¶é‡å»ºå›¾åƒè¡¥ä¸ä¸æ–‡æœ¬ã€‚</li>
          <!-- (7) ptp.png -->
          <figure class="mt-4">
            <img src="https://csu-jpg.github.io/Blog/figures/ptp.png" alt="PTP screenshot language models">
            <figcaption class="mt-2 text-xs text-neutral-500">PTP (arXiv'24): æˆªå›¾ä¸Šçš„è”åˆè¡¥ä¸å’Œæ–‡æœ¬é¢„æµ‹ã€‚</figcaption>
          </figure>
        </ul>
        <b>Fuyu:</b>é€šè¿‡Unified Modelå¤„ç†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/fuyu.png" alt="Fuyu: precess visual and textual information in a unified model.">
          <figcaption class="mt-2 text-xs text-neutral-500">Fuyu (arXiv 24'1):é€šè¿‡Unified Modelå¤„ç†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚</figcaption>
        </figure>
        <li><strong>PEAP</strong>ï¼ˆLyu ç­‰ï¼ŒarXiv 2025')æå‡ºä¸€ç§ç»Ÿä¸€çš„æ„ŸçŸ¥èŒƒå¼ï¼Œç”¨äºä¸çœŸå®ä¸–ç•Œç¯å¢ƒç›´æ¥äº¤äº’çš„æ™ºèƒ½è¯­è¨€æ¨¡å‹ï¼Œç»“åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚</li>
          <!-- (8) peap.png -->
          <figure class="mt-4">
            <img src="https://csu-jpg.github.io/Blog/figures/peap.png" alt="PEAP: a unified perception paradigm for agentic language models that interact directly with real-world environments combining visual and textual information.">
            <figcaption class="mt-2 text-xs text-neutral-500">PEAP (arXiv'25): ä¸€ç§ç»Ÿä¸€çš„æ„ŸçŸ¥èŒƒå¼ï¼Œç”¨äºä¸çœŸå®ä¸–ç•Œç¯å¢ƒç›´æ¥äº¤äº’çš„æ™ºèƒ½è¯­è¨€æ¨¡å‹ï¼Œç»“åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚</figcaption>
          </figure>
        <p>
        
          è¿™äº›å·¥ä½œè®©æ¨¡å‹å¼€å§‹"çœ‹è§"æ–‡æœ¬ï¼Œä½†ä¸€ä¸ªå…³é”®é—®é¢˜ä»å¾…å›ç­”ï¼šæŠŠæ–‡æœ¬å˜æˆå›¾åƒï¼Œ<em style="color: #22c55e; font-weight: bold;">å…·ä½“å¸¦æ¥ä»€ä¹ˆå¯é‡åŒ–æ”¶ç›Š</em>ï¼Ÿ
        </p>

        <h2>3. è¿‘æœŸè¿›å±•ï¼šç”¨è§†è§‰ token åšé•¿ä¸Šä¸‹æ–‡å‹ç¼©</h2>
        <p><strong>å…³é”®è§‚å¯Ÿï¼š</strong></p>
        <ul>
          <li>è§†è§‰ç¼–ç å™¨é€šå¸¸è¿œå°äº LLMï¼ˆå¦‚ ViT-B ~1e8 å‚æ•° vs. LLaMA/Mistral 7B+ï¼‰ã€‚</li>
          <li>CLIP å¼é¢„è®­ç»ƒèƒ½æ¶Œç°ç±» OCR èƒ½åŠ›ï¼Œå³ä½¿æ²¡æœ‰æ˜¾å¼ç›‘ç£ã€‚</li>
          <li>è§†è§‰ patch èƒ½ç¼–ç é«˜å¯†åº¦æ–‡æœ¬ï¼ˆå•ä½é¢ç§¯æ›´å¤šå­—ç¬¦ï¼‰ï¼Œé€šè¿‡<em>ç©ºé—´å‹ç¼©</em>æœ‰æ•ˆå»¶é•¿ä¸Šä¸‹æ–‡ã€‚</li>
        </ul>
        <p>
          äº¤é”™å¼ï¼ˆinterleavedï¼‰æ–‡æ¡£çº§å¤šæ¨¡æ€é¢„è®­ç»ƒæ˜¯ç†æƒ³è®¾å®šã€‚<br>
          <strong>NeurIPS 2024 â€”â€” "Leveraging Visual Tokens for Extended Text Contexts"ï¼š</strong>
          å°†é•¿æ–‡æœ¬è¡¨ç¤ºä¸ºç´§å‡‘çš„è§†è§‰ tokenï¼Œä½¿è®­ç»ƒä¸æ¨ç†éƒ½èƒ½å¤„ç†æ›´é•¿ã€æ›´å¯†é›†çš„ä¸Šä¸‹æ–‡ã€‚
          è¯¥å·¥ä½œæŠŠé¢„è®­ç»ƒé˜¶æ®µçš„å¯ç”¨æ–‡æœ¬ä¸Šä¸‹æ–‡ä» <strong>256 â†’ 2048</strong>ï¼ˆNVIDIA H100ï¼‰æ˜¾è‘—æå‡ã€‚
        </p>
            <!-- (9) vis_in_context.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vis_in_context.png" alt="Visual tokens for extended in-context learning">
          <figcaption class="mt-2 text-xs text-neutral-500">åˆ©ç”¨è§†è§‰ token è¿›è¡Œæ‰©å±•çš„ä¸Šä¸‹æ–‡ç†è§£ã€‚</figcaption>
        </figure>
                <!-- (10) vis_in_context_h100.png -->
                <figure class="mt-4">
                  <img src="https://csu-jpg.github.io/Blog/figures/visual_in_context_h100.png" alt="H100 pretraining: text context 256â†’2048">
                  <figcaption class="mt-2 text-xs text-neutral-500">åœ¨ NVIDIA H100 ä¸Šï¼šä¸Šä¸‹æ–‡æ–‡æœ¬é•¿åº¦ä» 256 â†’ 2048 æå‡ã€‚</figcaption>
                </figure>

        <p>
          é•¿æ–‡æœ¬ç†è§£æ˜¯å¦ä¸€ç†æƒ³åº”ç”¨ã€‚<br>
          <strong>Xin ç­‰ï¼ŒNeurIPS 2025 â€”â€” "Vision-Centric Token Compression in Large Language Models"ï¼ˆVISTï¼‰ï¼š</strong>
          å—äººç±»å¿«/æ…¢é€šè·¯é˜…è¯»å¯å‘ï¼š<em>å¿«é€šè·¯</em>æŠŠè¿œè·ä¸Šä¸‹æ–‡æ¸²æŸ“æˆå›¾åƒä»¥å¿«é€ŸæŠ½å–è¯­ä¹‰ï¼›<em>æ…¢é€šè·¯</em>æŠŠå…³é”®æ–‡æœ¬ç›´æ¥é€å…¥ LLM åšæ·±åº¦æ¨ç†ã€‚
        </p>
        
        <!-- (11) vist.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/vist.png" alt="VIST: vision-centric token compression in LLMs">
          <figcaption class="mt-2 text-xs text-neutral-500">VIST (NeurIPS'25): å¿«é€Ÿè§†è§‰é€šè·¯ + æ…¢é€Ÿè¯­è¨€é€šè·¯ã€‚</figcaption>
        </figure>

        <p>
          <strong>DeepSeek-OCRï¼ˆ2025 å¹´ 10 æœˆï¼‰ï¼šContextual Optical Compression</strong> æŠŠè§†è§‰ token å‹ç¼©æ‰©å±•åˆ°å¤§è§„æ¨¡ OCRï¼š
          å°†æ•°åƒæ–‡æœ¬ token å‹ç¼©ä¸ºæ•°ç™¾ä¸ªè§†è§‰è¡¨å¾ï¼Œè¾¾åˆ°çº¦ <strong>97% OCR ç²¾åº¦ã€çº¦ 10Ã— å‹ç¼©</strong>ã€‚
          DeepSeek-OCR çš„æˆåŠŸå¾—ç›Šäº <strong>å¼ºå¤§çš„åŸºç¡€è®¾æ–½</strong> å’Œ <strong>ç²¾ç»†çš„å¤§è§„æ¨¡æ•°æ®å‡†å¤‡</strong>ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šä»¥å¾€çš„å·¥ä½œï¼Œå®ç°è¿œè¶…é¢„æœŸçš„è§†è§‰â€“token å‹ç¼©æ•ˆæœã€‚
        </p>
        <!-- (12) deepseek_motivation.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/deepseek_motivation.png" alt="DeepSeek-OCR contextual optical compression">
          <figcaption class="mt-2 text-xs text-neutral-500">DeepSeek-OCR: çº¦ 97% ç²¾åº¦ï¼Œçº¦ 10Ã— å‹ç¼©ã€‚</figcaption>
        </figure>

        <p><em>è§†è§‰æ„ŸçŸ¥ä¸è¯­è¨€ç†è§£çš„èåˆå¹¶éå¶ç„¶â€”â€”å®ƒæ­£åœ¨æˆä¸ºä¸‹ä¸€ä¸ªèŒƒå¼ã€‚</em></p>



        <h2>4. æœªæ¥å›¾æ™¯ï¼šVision-centric MLLM</h2>
        <p>
          åœ¨çœŸæ­£ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹é‡Œï¼Œæˆ‘ä»¬æˆ–è®¸ä¸å†éœ€è¦ä¼ ç»Ÿ tokenizerã€‚
          æ¨¡å‹å°†åƒäººä¸€æ ·â€œè§†è§‰åŒ–é˜…è¯»â€ï¼Œç”šè‡³ä»¥å›¾åƒæ–¹å¼â€œè§†è§‰åŒ–ç”Ÿæˆâ€æ–‡æœ¬ï¼ŒäºåŒä¸€è§†è§‰ç©ºé—´ç»Ÿä¸€æ„ŸçŸ¥ä¸åˆ›ä½œã€‚
        </p>
        <p>
          ä¸ºæ­¤æˆ‘ä»¬éœ€è¦å®Œå–„åŸºäºå›¾åƒçš„æ–‡æœ¬æ¸²æŸ“ä¸é•¿æ–‡æœ¬è§†è§‰ç”Ÿæˆï¼š
          <strong>TextAtlas5M</strong> æä¾›å¤§è§„æ¨¡è‡´å¯†æ–‡æœ¬æ¸²æŸ“æ•°æ®ï¼Œè¦†ç›–è¯´æ˜ã€æ–‡æ¡£ä¸è®¾è®¡ç­‰è§†è§‰è¡¨è¾¾ï¼›
        </p>

        <span style="color:#d32f2f; font-weight:bold;"><u><b> ç¨ å¯†æ–‡æœ¬å›¾åƒç”Ÿæˆ:</b></u></span>
              <!-- (13) textatlas_5m.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/text_atlas_5m.png" alt="TextAtlas5M dataset for dense text rendering">
          <figcaption class="mt-2 text-xs text-neutral-500">TextAtlas5M: å¤§è§„æ¨¡è‡´å¯†æ–‡æœ¬æ¸²æŸ“ã€‚Arxiv 25'2ã€‚</figcaption>
        </figure>

        <br><em>Beyond Words</em> æ—¨åœ¨ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆé«˜æ–‡æœ¬é‡ã€é«˜ä¿¡æ¯å¯†åº¦çš„å›¾åƒï¼Œæ¨åŠ¨å¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹è¿ˆå‘çœŸæ­£çš„é•¿æ–‡æœ¬è§†è§‰ç”Ÿæˆã€‚</br>
           <!-- (14) long_text_ar.png -->
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/long_text_ar.png" alt="Beyond Words: long-text visual generation">
          <figcaption class="mt-2 text-xs text-neutral-500"><em>Beyond Words</em>: è¿ˆå‘é•¿æ–‡æœ¬è§†è§‰ç”Ÿæˆã€‚Arxiv 25'2ã€‚</figcaption>
        </figure>


        <span style="color:#d32f2f; font-weight:bold;"><u><b> æ¢ç´¢LLMé‡Œé¢æ›´å¤šä»»åŠ¡:</b></u></span>
        <p>
          <b><strong></strong>Xing et al, Arxiv25'10 - "SEE THE TEXT: FROM TOKENIZATION TO VISUAL READING"</strong></b>, è¿™ä¸ªå·¥ä½œå°†è§†è§‰å¤„ç†æ–‡æœ¬çš„æ€æƒ³æ‰©å±•åˆ°æ›´å¤šä»»åŠ¡åŒ…æ‹¬åˆ†ç±»å’ŒQA.
        </p>
        <figure class="mt-4">
          <img src="https://csu-jpg.github.io/Blog/figures/see_the_text.png" alt="See The Text">
          <figcaption class="mt-2 text-xs text-neutral-500">See The Text (Arxiv25'10): è§†è§‰å¤„ç†æ–‡æœ¬çš„æ€æƒ³æ‰©å±•åˆ°æ›´å¤šä»»åŠ¡åŒ…æ‹¬åˆ†ç±»å’ŒQA.</figcaption>
        </figure>

        <h2>5. èµ°å‘ä¸‹ä¸€ä»£ Vision-centric MLLM</h2>
        <ul>
          <li>æ–‡æœ¬é¦–å…ˆæ˜¯<strong>è§†è§‰</strong>ï¼Œè€Œéä»…æ˜¯ç¬¦å·ã€‚</li>
          <li>è§†è§‰å°±æ˜¯<strong>è¯­è¨€</strong>ï¼Œä¸¤è€…å¹¶ä¸å‰²è£‚ã€‚</li>
          <li>å‹ç¼©å³æ˜¯<strong>æ„ŸçŸ¥</strong>ï¼Œè€Œéä»…æ˜¯å·¥ç¨‹æ‰‹æ®µã€‚</li>
        </ul>
        <p>
          ç»ˆæç›®æ ‡ï¼šè®©æ¨¡å‹åƒäººç±»ä¸€æ ·å»è¯»ã€å»å†™ã€å»â€œçœ‹è§â€æ–‡æœ¬ã€‚<br/>
          <strong>People see text. å¾ˆå¿«ï¼ŒLLMs ä¸ LVMs ä¹Ÿä¼šå¦‚æ­¤ã€‚</strong>
        </p>
        <!-- <p>
          æˆ‘ä»¬æ•´ç†äº†è§†è§‰æ–‡æœ¬ç›¸å…³è®ºæ–‡åˆ—è¡¨ï¼šè¯·è®¿é—® 
          <a href="https://github.com/CSU-JPG/Awesome-Visual-Text" target="_blank" rel="noopener noreferrer">https://github.com/CSU-JPG/Awesome-Visual-Text</a>
          æŸ¥çœ‹è¯¦æƒ…ã€‚
        </p> -->
      </article>

      <hr class="my-10 border-neutral-200"/>
      <div class="flex items-center justify-between text-sm">
        <a id="backHome" href="../" class="text-blue-700 hover:underline">â† Back to Home</a>
        <a id="backTop" href="#top" class="text-neutral-500 hover:underline">Back to Top â†‘</a>
      </div>
    </main>

    <footer class="border-t border-neutral-200 py-10 text-center text-sm text-neutral-500">
      Â© 2025 CSU-JPG Lab Â· <a class="hover:underline" href="https://csu-jpg.github.io" target="_blank" rel="noreferrer">https://csu-jpg.github.io</a>
    </footer>

    <!-- ===== è¯­è¨€åˆ‡æ¢ + ç›®å½•ç”Ÿæˆ ===== -->
    <script>
      const LS_KEY = 'csujpg_lang';
      const getLang = () => localStorage.getItem(LS_KEY) || 'en';
      const setLang = (l) => { localStorage.setItem(LS_KEY, l); document.documentElement.lang = (l==='zh'?'zh-CN':'en'); };

      const $en = document.getElementById('content-en');
      const $zh = document.getElementById('content-zh');
      const $toc = document.getElementById('toc');
      const $btnEN = document.getElementById('btnEN');
      const $btnZH = document.getElementById('btnZH');

      const UI_TEXT = {
        en: { home:'â† Home', pub:'Published on 2025-10-21 Â· CSU-JPG Lab', toc:'Outline', backHome:'â† Back to Home', backTop:'Back to Top â†‘' },
        zh: { home:'â† è¿”å›é¦–é¡µ', pub:'å‘å¸ƒäº 2025-10-21 Â· CSU-JPG Lab', toc:'ç›®å½•', backHome:'â† è¿”å›é¦–é¡µ', backTop:'å›åˆ°é¡¶éƒ¨ â†‘' }
      };

      function buildTOC() {
        if (!$toc) return;
        const active = !$en.hidden ? $en : $zh;
        const hs = active.querySelectorAll('h1, h2, h3');
        const ol = document.createElement('ol');
        ol.className = 'list-decimal list-inside mt-2 space-y-1 text-blue-700';
        hs.forEach((h, i) => {
          if (!h.id) {
            h.id = 'sec-' + i + '-' + (h.textContent||'').trim().toLowerCase().replace(/\s+/g,'-').replace(/[^\w\-]/g,'');
          }
          const li = document.createElement('li');
          const a = document.createElement('a');
          a.href = '#' + h.id;
          a.textContent = h.textContent;
          a.className = 'hover:underline';
          li.appendChild(a);
          ol.appendChild(li);
        });
        const lang = (!$en.hidden ? 'en' : 'zh');
        $toc.innerHTML = '';
        const title = document.createElement('div');
        title.className = 'font-semibold';
        title.textContent = UI_TEXT[lang].toc;
        $toc.appendChild(title);
        $toc.appendChild(ol);
      }

      function applyLang(l){
        setLang(l);
        if (l === 'zh') {
          $en.hidden = true;  $zh.hidden = false;
          $btnEN.className = 'px-2 py-1 rounded hover:bg-neutral-100';
          $btnZH.className = 'px-2 py-1 rounded bg-black text-white';
        } else {
          $en.hidden = false; $zh.hidden = true;
          $btnEN.className = 'px-2 py-1 rounded bg-black text-white';
          $btnZH.className = 'px-2 py-1 rounded hover:bg-neutral-100';
        }
        const t = UI_TEXT[l];
        document.getElementById('homeLink').textContent = t.home;
        document.getElementById('pubdate').textContent = t.pub;
        document.getElementById('backHome').textContent = t.backHome;
        document.getElementById('backTop').textContent = t.backTop;
        buildTOC();
      }

      // åˆå§‹åŒ–
      applyLang(getLang());
      // ç»‘å®šæŒ‰é’®
      $btnEN.addEventListener('click', ()=>applyLang('en'));
      $btnZH.addEventListener('click', ()=>applyLang('zh'));
    </script>
  </body>
</html>
