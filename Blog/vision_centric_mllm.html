<!doctype html>
<html lang="en"> <!-- é»˜è®¤è‹±æ–‡ -->
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Stories | Vision-centric MLLM Evolution</title>
    <meta name="description" content="CSU-JPG: Research story and evolution around Leveraging Visual Tokens (NeurIPS'24) and Vision-centric Token Compression (NeurIPS'25 Spotlight)." />
    <meta property="og:title" content="Stories | Vision-centric MLLM Evolution" />
    <meta property="og:description" content="From visualizing long text to vision-fast-scan + language-deep-read, our two-step route for Vision-centric MLLM." />
    <meta property="og:type" content="article" />
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='0.9em' font-size='90'>ğŸ§ </text></svg>">
    <script src="https://cdn.tailwindcss.com"></script>
    <style>html{scroll-behavior:smooth}</style>
  </head>
  <body class="bg-white text-neutral-900">
    <header class="border-b border-neutral-200">
      <div class="max-w-4xl mx-auto px-4 py-4 flex items-center justify-between">
        <a id="homeLink" href="https://csu-jpg.github.io/" class="text-sm text-blue-700 hover:underline">â† Home</a>
        <div class="flex items-center gap-3">
          <div id="pubdate" class="text-sm text-neutral-500">Published on 2025-10-21 Â· CSU-JPG Lab</div>
          <div class="h-5 w-px bg-neutral-200"></div>
          <div class="text-sm">
            <button id="btnEN" class="px-2 py-1 rounded bg-black text-white">EN</button>
            <span class="mx-1 text-neutral-300">|</span>
            <button id="btnZH" class="px-2 py-1 rounded hover:bg-neutral-100">ä¸­æ–‡</button>
          </div>
        </div>
      </div>
    </header>

    <main class="max-w-4xl mx-auto px-4 py-10">
      <h1 id="title" class="text-3xl md:text-4xl font-extrabold leading-tight">
        Vision-centric MLLM: The Technical Evolution
      </h1>
      <p id="subtitle" class="mt-3 text-neutral-600">
        This article tells our two-step route for advancing MLLMs with a <strong>visual-first</strong> approach:
        <span class="whitespace-nowrap">â‘  NeurIPS'24: Leveraging Visual Tokens</span> and
        <span class="whitespace-nowrap">â‘¡ NeurIPS'25 Spotlight: Vision-centric Token Compression</span>.
      </p>

      <!-- Hero Figure -->
      <figure class="mt-6">
        <img class="w-full rounded-2xl border" src="https://fingerrec.github.io/index_files/jinpeng/papers/ARXIV2024/motivation.png" alt="NeurIPS 2024 Visual Tokens Motivation"/>
        <figcaption id="fig1cap" class="mt-2 text-xs text-neutral-500">
          Fig.1: Visualize long text so the vision encoder handles layout & dense info, relieving LLM context pressure (NeurIPS'24).
        </figcaption>
      </figure>

      <!-- Outline -->
      <nav class="mt-8 text-sm">
        <div id="tocTitle" class="font-semibold">Outline</div>
        <ol class="list-decimal list-inside mt-2 space-y-1 text-blue-700">
          <li><a id="toc1" class="hover:underline" href="#motivation">Motivation: Why â€œvisualize textâ€?</a></li>
          <li><a id="toc2" class="hover:underline" href="#nips24">NeurIPS'24: Leveraging Visual Tokens for Extended Text Contexts</a></li>
          <li><a id="toc3" class="hover:underline" href="#nips25">NeurIPS'25 Spotlight: Vision-centric Token Compression in LLM</a></li>
          <li><a id="toc4" class="hover:underline" href="#compare">Contrast & Evolution: How the two steps connect</a></li>
          <li><a id="toc5" class="hover:underline" href="#related">Related Evidence & References</a></li>
        </ol>
      </nav>

      <h2>4. æœªæ¥å›¾æ™¯ï¼šVision-centric MLLM</h2>
      <p>
        åœ¨çœŸæ­£ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹é‡Œï¼Œæˆ‘ä»¬æˆ–è®¸ä¸å†éœ€è¦ä¼ ç»Ÿ tokenizerã€‚
        æ¨¡å‹å°†åƒäººä¸€æ ·â€œè§†è§‰åŒ–é˜…è¯»â€ï¼Œç”šè‡³ä»¥å›¾åƒæ–¹å¼â€œè§†è§‰åŒ–ç”Ÿæˆâ€æ–‡æœ¬ï¼ŒäºåŒä¸€è§†è§‰ç©ºé—´ç»Ÿä¸€æ„ŸçŸ¥ä¸åˆ›ä½œã€‚
      </p>
      <p>
        ä¸ºæ­¤æˆ‘ä»¬éœ€è¦å®Œå–„åŸºäºå›¾åƒçš„æ–‡æœ¬æ¸²æŸ“ä¸é•¿æ–‡æœ¬è§†è§‰ç”Ÿæˆï¼š
        <strong>TextAtlas5M</strong> æä¾›å¤§è§„æ¨¡è‡´å¯†æ–‡æœ¬æ¸²æŸ“æ•°æ®ï¼Œè¦†ç›–è¯´æ˜ã€æ–‡æ¡£ä¸è®¾è®¡ç­‰è§†è§‰è¡¨è¾¾ï¼›
        <br><em>Beyond Words</em> æ—¨åœ¨ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆé«˜æ–‡æœ¬é‡ã€é«˜ä¿¡æ¯å¯†åº¦çš„å›¾åƒï¼Œæ¨åŠ¨å¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹è¿ˆå‘çœŸæ­£çš„é•¿æ–‡æœ¬è§†è§‰ç”Ÿæˆã€‚
      </p>
            <!-- (13) textatlas_5m.png -->
      <figure class="mt-4">
        <img src="https://csu-jpg.github.io/Blog/figures/text_atlas_5m.png" alt="TextAtlas5M dataset for dense text rendering">
        <figcaption class="mt-2 text-xs text-neutral-500">TextAtlas5M: å¤§è§„æ¨¡è‡´å¯†æ–‡æœ¬æ¸²æŸ“ã€‚Arxiv 25'2ã€‚</figcaption>
      </figure>
         <!-- (14) long_text_ar.png -->
      <figure class="mt-4">
        <img src="https://csu-jpg.github.io/Blog/figures/long_text_ar.png" alt="Beyond Words: long-text visual generation">
        <figcaption class="mt-2 text-xs text-neutral-500"><em>Beyond Words</em>: è¿ˆå‘é•¿æ–‡æœ¬è§†è§‰ç”Ÿæˆã€‚Arxiv 25'2ã€‚</figcaption>
      </figure>

      <h2>4. The Future Ahead: A Vision-centric MLLM</h2>
      <p>
        In a truly vision-centric multimodal language model, we may no longer need a traditional tokenizer.
        The model could read text visually â€” as humans do â€” and even generate text as images, <b> <u>unifying perception and generation in the same visual space.</u> </b>
      </p>
      <b> Dense Text Image Generation:</b>
      <p>
        To reach that goal, we must perfect image-based text rendering and long-text visual generation:
        <strong>TextAtlas5M</strong> provides large-scale dense text rendering where captions, documents, and designs are visually represented.
            <!-- (13) textatlas_5m.png -->
      <figure class="mt-4">
        <img src="https://csu-jpg.github.io/Blog/figures/text_atlas_5m.png" alt="TextAtlas5M dataset for dense text rendering">
        <figcaption class="mt-2 text-xs text-neutral-500">TextAtlas5M: large-scale dense text rendering. Arxiv 25'2.</figcaption>
      </figure>
        <br><em>Beyond Words</em> aims to generate text-heavy, information-dense images from natural prompts, pushing multimodal autoregressive models toward true long-text visual generation.
         <!-- (14) long_text_ar.png -->
      <figure class="mt-4">
        <img src="https://csu-jpg.github.io/Blog/figures/long_text_ar.png" alt="Beyond Words: long-text visual generation">
        <figcaption class="mt-2 text-xs text-neutral-500"><em>Beyond Words</em>: toward long-text visual generation. Arxiv 25'2.</figcaption>
      </figure>
      </p>
      <section id="motivation" class="mt-10">
        <h2 id="motivationTitle" class="text-2xl font-bold">Motivation: Why â€œvisualize textâ€?</h2>
        <p id="motivationBody" class="mt-3 leading-7 text-neutral-700">
          In real scenarios, long text is often <strong>mixed-layout</strong>, with <strong>dense characters</strong> and <strong>cross-region relations</strong>.
          Processing it directly with an LLM consumes context budget and strains layout understanding and cross-paragraph integration.
          Vision encoders excel at <strong>spatial structure</strong>, <strong>layout</strong> and <strong>scanning</strong>:
          by rendering long text into images or visual tokens, the vision model can â€œscan firstâ€ and pass only key information to the LLM, enabling
          <strong>longer effective context, higher efficiency, and more robust understanding</strong>.
        </p>
      </section>

      <section id="nips24" class="mt-10">
        <h2 id="n24Title" class="text-2xl font-bold">NeurIPS'24: Leveraging Visual Tokens for Extended Text Contexts</h2>
        <p id="n24Body" class="mt-3 leading-7 text-neutral-700">
          In 2024, we proposed to <strong>compress long text with a vision encoder</strong>:
          â€œvisualize textâ€ and encode it as visual tokens, then reason with an LLM. This significantly extends effective context and is
          friendly to complex layouts, tables and screenshot-like inputs.
        </p>
        <ul id="n24Bullets" class="mt-3 list-disc list-inside text-neutral-700 space-y-1">
          <li>Intuition: save <em>context budget</em>, stabilize <em>cross-region associations</em>.</li>
          <li>Typical scenarios: manuals/webpages/paper screenshots, cross-paragraph QA, retrieval & reasoning with complex layout.</li>
        </ul>
        <figure class="mt-4">
          <img class="w-full rounded-2xl border" src="https://fingerrec.github.io/index_files/jinpeng/papers/longtextar2025/main_ppl.png" alt="LongTextAR Figure"/>
          <figcaption id="fig2cap" class="mt-2 text-xs text-neutral-500">
            Fig.2: Works like LongTextAR supplement training/evaluation for dense text image generation/understanding.
          </figcaption>
        </figure>
        <div class="mt-3 text-sm text-blue-700 space-x-4">
          <a id="n24Paper" class="hover:underline" href="https://arxiv.org/abs/2406.02547" target="_blank" rel="noreferrer">Paper (arXiv:2406.02547)</a>
          <a id="n24Code" class="hover:underline" href="https://github.com/showlab/visincontext" target="_blank" rel="noreferrer">Code</a>
          <a id="n24Proj" class="hover:underline" href="https://fingerrec.github.io/visincontext/" target="_blank" rel="noreferrer">Project</a>
        </div>
      </section>

      <section id="nips25" class="mt-10">
        <h2 id="n25Title" class="text-2xl font-bold">NeurIPS'25 Spotlight: Vision-centric Token Compression in LLM</h2>
        <p id="n25Body" class="mt-3 leading-7 text-neutral-700">
          In 2025, we further propose a <strong>reading heuristic</strong>:
          humans <em>scan</em> unimportant content visually and <em>deep-read</em> key content with language.
          Accordingly, we <strong>delegate non-critical content to the vision encoder for compression</strong>,
          reserving precious LLM context for core partsâ€”achieving lower latency and stronger long-text handling.
        </p>
        <ul id="n25Bullets" class="mt-3 list-disc list-inside text-neutral-700 space-y-1">
          <li>Vision: fast scanning, structure extraction, redundancy compression.</li>
          <li>Language: key details, logical reasoning, high-level planning.</li>
          <li>Outcome: comparable or better performance with equal/smaller context.</li>
        </ul>
        <div class="mt-3 text-sm text-blue-700 space-x-4">
          <a id="n25Paper" class="hover:underline" href="https://arxiv.org/abs/2502.00791" target="_blank" rel="noreferrer">Paper (preprint)</a>
        </div>
      </section>

      <section id="compare" class="mt-10">
        <h2 id="cmpTitle" class="text-2xl font-bold">Contrast & Evolution: How the two steps connect</h2>
        <div class="mt-3 grid md:grid-cols-2 gap-4 text-sm">
          <div class="rounded-2xl border p-4">
            <div id="cmpL" class="font-semibold">Step 1 (NeurIPS'24)</div>
            <ul id="cmpLBullets" class="mt-2 list-disc list-inside space-y-1 text-neutral-700">
              <li>Visualize text â†’ compress long context via visual tokens</li>
              <li>Strengthen layout & dense-text understanding</li>
              <li>Significantly reduce LLM context usage</li>
            </ul>
          </div>
          <div class="rounded-2xl border p-4">
            <div id="cmpR" class="font-semibold">Step 2 (NeurIPS'25 Spotlight)</div>
            <ul id="cmpRBullets" class="mt-2 list-disc list-inside space-y-1 text-neutral-700">
              <li>Reading heuristic: vision fast-scan, language deep-read</li>
              <li>Dynamic context budgeting, more efficient</li>
              <li>End-to-end task performance continues improving</li>
            </ul>
          </div>
        </div>
      </section>

      <section id="related" class="mt-10">
        <h2 id="relTitle" class="text-2xl font-bold">Related Evidence & References</h2>
        <p id="relBody" class="mt-3 leading-7 text-neutral-700">
          The â€œvisualizing textâ€ idea is supported by several parallel lines of work in the community:
        </p>
        <ul id="relList" class="mt-2 list-disc list-inside text-neutral-700 space-y-1">
          <li>Princeton: <em>Language Modelling with Pixels</em> (ICLR 2023)</li>
          <li>Google Research: <em>CLIPPO: Image-and-Language Understanding from Pixels Only</em> (CVPR 2023)</li>
          <li>Lee et al.: <em>Pix2Struct</em> (ICML 2023)</li>
          <li>Danqi Chenâ€™s group: <em>Improving Language Understanding from Screenshots</em> (ArXiv 2024/02)</li>
        </ul>
        <p id="relMore" class="mt-3 text-sm text-blue-700">
          Further reading:
          <a class="hover:underline" href="https://textatlas5m.github.io/" target="_blank" rel="noreferrer">TextAtlas5M</a> ,
          <a class="hover:underline" href="https://github.com/CSU-JPG/V-MAGE" target="_blank" rel="noreferrer">V-MAGE</a> ,
          <a class="hover:underline" href="https://github.com/CSU-JPG/MVPBench" target="_blank" rel="noreferrer">MVPBench</a>
        </p>
      </section>

      <hr class="my-10 border-neutral-200"/>
      <div class="flex items-center justify-between text-sm">
        <a id="backHome" href="./" class="text-blue-700 hover:underline">â† Back to Home</a>
        <a id="backTop" href="#top" class="text-neutral-500 hover:underline">Back to Top â†‘</a>
      </div>
    </main>

    <footer class="border-t border-neutral-200 py-10 text-center text-sm text-neutral-500">
      Â© 2025 CSU-JPG Lab Â· <a class="hover:underline" href="https://csu-jpg.github.io" target="_blank" rel="noreferrer">https://csu-jpg.github.io</a>
    </footer>

    <script>
      // ===== ç®€æ˜“ i18nï¼ˆé»˜è®¤è‹±æ–‡ï¼‰ï¼Œä¸é¦–é¡µä¿æŒä¸€è‡´çš„ localStorage é”® =====
      const LS_KEY = 'csujpg_lang';
      const getLang = () => localStorage.getItem(LS_KEY) || 'en';
      const setLang = (l) => { localStorage.setItem(LS_KEY, l); document.documentElement.lang = (l==='en'?'en':'zh-CN'); };

      const T = {
        en: {
          home: 'â† Home',
          pubdate: 'Published on 2025-10-21 Â· CSU-JPG Lab',
          title: 'Vision-centric MLLM: The Technical Evolution',
          subtitle: 'This article tells our two-step route for advancing MLLMs with a <strong>visual-first</strong> approach: <span class="whitespace-nowrap">â‘  NeurIPS\'24: Leveraging Visual Tokens</span> and <span class="whitespace-nowrap">â‘¡ NeurIPS\'25 Spotlight: Vision-centric Token Compression</span>.',
          fig1cap: 'Fig.1: Visualize long text so the vision encoder handles layout & dense info, relieving LLM context pressure (NeurIPS\'24).',
          tocTitle: 'Outline',
          toc1: 'Motivation: Why â€œvisualize textâ€?',
          toc2: 'NeurIPS\'24: Leveraging Visual Tokens for Extended Text Contexts',
          toc3: 'NeurIPS\'25 Spotlight: Vision-centric Token Compression in LLM',
          toc4: 'Contrast & Evolution: How the two steps connect',
          toc5: 'Related Evidence & References',
          motivationTitle: 'Motivation: Why â€œvisualize textâ€?',
          motivationBody: 'In real scenarios, long text is often <strong>mixed-layout</strong>, with <strong>dense characters</strong> and <strong>cross-region relations</strong>. Processing it directly with an LLM consumes context budget and strains layout understanding and cross-paragraph integration. Vision encoders excel at <strong>spatial structure</strong>, <strong>layout</strong> and <strong>scanning</strong>: by rendering long text into images or visual tokens, the vision model can â€œscan firstâ€ and pass only key information to the LLM, enabling <strong>longer effective context, higher efficiency, and more robust understanding</strong>.',
          n24Title: 'NeurIPS\'24: Leveraging Visual Tokens for Extended Text Contexts',
          n24Body: 'In 2024, we proposed to <strong>compress long text with a vision encoder</strong>: â€œvisualize textâ€ and encode it as visual tokens, then reason with an LLM. This significantly extends effective context and is friendly to complex layouts, tables and screenshot-like inputs.',
          n24Bullets: [
            'Intuition: save <em>context budget</em>, stabilize <em>cross-region associations</em>.',
            'Typical scenarios: manuals/webpages/paper screenshots, cross-paragraph QA, retrieval & reasoning with complex layout.'
          ],
          fig2cap: 'Fig.2: Works like LongTextAR supplement training/evaluation for dense text image generation/understanding.',
          n24Paper: 'Paper (arXiv:2406.02547)',
          n24Code: 'Code',
          n24Proj: 'Project',
          n25Title: 'NeurIPS\'25 Spotlight: Vision-centric Token Compression in LLM',
          n25Body: 'In 2025, we further propose a <strong>reading heuristic</strong>: humans <em>scan</em> unimportant content visually and <em>deep-read</em> key content with language. Accordingly, we <strong>delegate non-critical content to the vision encoder for compression</strong>, reserving precious LLM context for core partsâ€”achieving lower latency and stronger long-text handling.',
          n25Bullets: [
            'Vision: fast scanning, structure extraction, redundancy compression.',
            'Language: key details, logical reasoning, high-level planning.',
            'Outcome: comparable or better performance with equal/smaller context.'
          ],
          n25Paper: 'Paper (preprint)',
          cmpTitle: 'Contrast & Evolution: How the two steps connect',
          cmpL: 'Step 1 (NeurIPS\'24)',
          cmpLBullets: [
            'Visualize text â†’ compress long context via visual tokens',
            'Strengthen layout & dense-text understanding',
            'Significantly reduce LLM context usage'
          ],
          cmpR: 'Step 2 (NeurIPS\'25 Spotlight)',
          cmpRBullets: [
            'Reading heuristic: vision fast-scan, language deep-read',
            'Dynamic context budgeting, more efficient',
            'End-to-end task performance continues improving'
          ],
          relTitle: 'Related Evidence & References',
          relBody: 'The â€œvisualizing textâ€ idea is supported by several parallel lines of work in the community:',
          relList: [
            'Princeton: <em>Language Modelling with Pixels</em> (ICLR 2023)',
            'Google Research: <em>CLIPPO: Image-and-Language Understanding from Pixels Only</em> (CVPR 2023)',
            'Lee et al.: <em>Pix2Struct</em> (ICML 2023)',
            'Danqi Chenâ€™s group: <em>Improving Language Understanding from Screenshots</em> (ArXiv 2024/02)'
          ],
          relMore: 'Further reading:',
          backHome: 'â† Back to Home',
          backTop: 'Back to Top â†‘'
        },
        zh: {
          home: 'â† è¿”å›é¦–é¡µ',
          pubdate: 'å‘å¸ƒäº 2025-10-21 Â· CSU-JPG Lab',
          title: 'ä»¥è§†è§‰ä¸ºä¸­å¿ƒï¼ˆVision-centricï¼‰çš„å¤šæ¨¡æ€å¤§æ¨¡å‹æ¼”è¿›',
          subtitle: 'æœ¬æ–‡è®²è¿°æˆ‘ä»¬å°†<strong>é•¿æ–‡æœ¬â€œè§†è§‰åŒ–â€</strong>ã€å¹¶ä»¥<strong>è§†è§‰ä¸ºå…ˆ</strong>æ¨è¿›å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰èƒ½åŠ›çš„ä¸¤æ­¥èµ°è·¯çº¿ï¼š<span class="whitespace-nowrap">â‘  NeurIPS\'24ï¼šLeveraging Visual Tokens</span> ä¸ <span class="whitespace-nowrap">â‘¡ NeurIPS\'25 Spotlightï¼šVision-centric Token Compression</span>ã€‚',
          fig1cap: 'å›¾ 1ï¼šå°†é•¿æ–‡æœ¬è§†è§‰åŒ–ï¼Œç”±è§†è§‰ç¼–ç å™¨å¤„ç†å¸ƒå±€ä¸å¯†é›†ä¿¡æ¯ï¼Œç¼“è§£ LLM ä¸Šä¸‹æ–‡å‹åŠ›ï¼ˆNeurIPS\'24ï¼‰ã€‚',
          tocTitle: 'ç›®å½•',
          toc1: 'åŠ¨æœºï¼šä¸ºä½•â€œæŠŠæ–‡æœ¬è§†è§‰åŒ–â€ï¼Ÿ',
          toc2: 'NeurIPS\'24ï¼šLeveraging Visual Tokens for Extended Text Contexts',
          toc3: 'NeurIPS\'25 Spotlightï¼šVision-centric Token Compression in LLM',
          toc4: 'å¯¹æ¯”ä¸æ¼”è¿›ï¼šä¸¤æ­¥èµ°å¦‚ä½•è¡”æ¥',
          toc5: 'ç›¸å…³è¯æ®ä¸å‚è€ƒ',
          motivationTitle: 'åŠ¨æœºï¼šä¸ºä½•â€œæŠŠæ–‡æœ¬è§†è§‰åŒ–â€ï¼Ÿ',
          motivationBody: 'åœ¨çœŸå®ä¸–ç•Œä¸­ï¼Œé•¿æ–‡æœ¬å¾€å¾€<strong>æ··æ’</strong>ã€å«æœ‰<strong>å¸ƒå±€</strong>ã€<strong>å¯†é›†å­—ç¬¦</strong>ä¸<strong>è·¨åŒºåŸŸå…³è”</strong>ã€‚ç›´æ¥è®©è¯­è¨€æ¨¡å‹å¤„ç†è¿™äº›è¾“å…¥ï¼Œä¸ä»…å ç”¨å®è´µçš„ä¸Šä¸‹æ–‡çª—å£ï¼Œä¹Ÿè®©æ¨¡å‹åœ¨ç‰ˆå¼ç†è§£ä¸è·¨æ®µä¿¡æ¯æ•´åˆä¸Šæ‰¿å‹ã€‚è§†è§‰ç¼–ç å™¨åœ¨<strong>ç©ºé—´ç»“æ„</strong>ã€<strong>æ’ç‰ˆ</strong>ä¸<strong>æ‰«æ</strong>æ–¹é¢å…·å¤‡å¤©ç„¶ä¼˜åŠ¿ï¼šå¦‚æœæˆ‘ä»¬å…ˆæŠŠé•¿æ–‡æœ¬æ¸²æŸ“ä¸ºå›¾åƒæˆ–è§†è§‰ tokenï¼Œè§†è§‰æ¨¡å‹å¯ä»¥å…ˆè¡Œâ€œæ‰«å›¾â€ï¼ŒåªæŠŠå…³é”®ä¿¡æ¯ä¼ é€’ç»™ LLMï¼Œä»è€Œå®ç°<strong>æ›´é•¿ä¸Šä¸‹æ–‡ã€æ›´é«˜æ•ˆç‡ã€æ›´ç¨³å¥çš„ç†è§£</strong>ã€‚',
          n24Title: 'NeurIPS\'24ï¼šLeveraging Visual Tokens for Extended Text Contexts',
          n24Body: 'æˆ‘ä»¬åœ¨ 2024 å¹´æå‡ºï¼š<strong>ç”¨è§†è§‰ç¼–ç å™¨å‹ç¼©é•¿æ–‡æœ¬</strong>ï¼Œå°†æ–‡æœ¬â€œè§†è§‰åŒ–â€å¹¶ç¼–ç ä¸ºè§†è§‰ tokenï¼Œç»“åˆè¯­è¨€æ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚è¿™ä¸€æ–¹æ¡ˆæ˜¾è‘—æ‹‰é•¿äº†æœ‰æ•ˆä¸Šä¸‹æ–‡ï¼Œå¹¶å¯¹å¤æ‚ç‰ˆå¼ã€è¡¨æ ¼ã€æˆªå›¾ç±»è¾“å…¥æ›´å‹å¥½ã€‚',
          n24Bullets: [
            'ç›´è§‚å¥½å¤„ï¼š<em>ä¸Šä¸‹æ–‡é¢„ç®—</em>æ›´çœã€<em>è·¨åŒºåŸŸå…³è”</em>æ›´ç¨³ã€‚',
            'å…¸å‹åœºæ™¯ï¼šè¯´æ˜ä¹¦/ç½‘é¡µ/è®ºæ–‡æˆªå›¾è§£æã€è·¨æ®µè½é—®ç­”ã€å¸¦å¤æ‚æ’ç‰ˆçš„æ£€ç´¢ä¸æ¨ç†ã€‚'
          ],
          fig2cap: 'å›¾ 2ï¼šLongTextAR ç­‰ç›¸å…³å·¥ä½œè¡¥å……äº†å¯†é›†æ–‡æœ¬å›¾åƒç”Ÿæˆ/ç†è§£çš„è®­ç»ƒä¸è¯„æµ‹ç»´åº¦ã€‚',
          n24Paper: 'è®ºæ–‡ï¼ˆarXiv:2406.02547ï¼‰',
          n24Code: 'ä»£ç ',
          n24Proj: 'é¡¹ç›®é¡µ',
          n25Title: 'NeurIPS\'25 Spotlightï¼šVision-centric Token Compression in LLM',
          n25Body: '2025 å¹´æˆ‘ä»¬è¿›ä¸€æ­¥æå‡º<strong>é˜…è¯»å¯å‘å¼</strong>ï¼š<em>äººç±»ä¼šç”¨â€œè§†è§‰â€å¿«é€Ÿæ‰«æä¸é‡è¦ä¿¡æ¯ã€ç”¨â€œè¯­è¨€â€ç²¾è¯»å…³é”®ä¿¡æ¯</em>ã€‚æ®æ­¤ï¼Œæˆ‘ä»¬å°†<strong>éå…³é”®ä¿¡æ¯äº¤ç»™è§†è§‰ç¼–ç å™¨å‹ç¼©</strong>ï¼ŒæŠŠå®è´µçš„è¯­è¨€ä¸Šä¸‹æ–‡ç•™ç»™æ ¸å¿ƒå†…å®¹ï¼Œå®ç°æ›´ä½å»¶è¿Ÿã€æ›´å¼ºé•¿æ–‡å¤„ç†èƒ½åŠ›ã€‚',
          n25Bullets: [
            'è§†è§‰ï¼šå¿«é€Ÿæ‰«æã€ç»“æ„æŠ½å–ã€å†—ä½™å‹ç¼©ã€‚',
            'è¯­è¨€ï¼šå…³é”®ç»†èŠ‚ã€é€»è¾‘æ¨ç†ã€é«˜å±‚è§„åˆ’ã€‚',
            'æ•ˆæœï¼šåœ¨åŒç­‰æˆ–æ›´å°ä¸Šä¸‹æ–‡ä¸‹ï¼Œä¿æŒ/æå‡å¤æ‚ä»»åŠ¡æ€§èƒ½ã€‚'
          ],
          n25Paper: 'è®ºæ–‡ï¼ˆé¢„å°æœ¬ï¼‰',
          cmpTitle: 'å¯¹æ¯”ä¸æ¼”è¿›ï¼šä¸¤æ­¥èµ°å¦‚ä½•è¡”æ¥',
          cmpL: 'Step 1ï¼ˆNeurIPS\'24ï¼‰',
          cmpLBullets: [
            'æŠŠæ–‡æœ¬è§†è§‰åŒ– â†’ è§†è§‰ token å‹ç¼©é•¿ä¸Šä¸‹æ–‡',
            'å¼ºåŒ–å¸ƒå±€ä¸å¯†é›†æ–‡æœ¬ç†è§£',
            'æ˜¾è‘—é™ä½è¯­è¨€ä¸Šä¸‹æ–‡å ç”¨'
          ],
          cmpR: 'Step 2ï¼ˆNeurIPS\'25 Spotlightï¼‰',
          cmpRBullets: [
            'é˜…è¯»å¯å‘å¼ï¼šè§†è§‰å¿«é€Ÿæ‰«ã€è¯­è¨€ç²¾è¯»',
            'åŠ¨æ€åˆ†é…ä¸Šä¸‹æ–‡é¢„ç®—ï¼Œæ›´é«˜æ•ˆ',
            'ç«¯åˆ°ç«¯ä»»åŠ¡æ€§èƒ½æŒç»­æå‡'
          ],
          relTitle: 'ç›¸å…³è¯æ®ä¸å‚è€ƒ',
          relBody: 'â€œæ–‡æœ¬è§†è§‰åŒ–â€çš„æ€è·¯åœ¨ç¤¾åŒºä¸­æœ‰å¤šæ¡å¹³è¡Œè¯æ®æ”¯æ’‘ï¼š',
          relList: [
            'Princetonï¼š<em>Language Modelling with Pixels</em>ï¼ˆICLR 2023ï¼‰',
            'Google Researchï¼š<em>CLIPPO: Image-and-Language Understanding from Pixels Only</em>ï¼ˆCVPR 2023ï¼‰',
            'Lee et al.ï¼š<em>Pix2Struct</em>ï¼ˆICML 2023ï¼‰',
            'é™ˆä¸¹å¥‡å›¢é˜Ÿï¼š<em>Improving Language Understanding from Screenshots</em>ï¼ˆArXiv 2024/02ï¼‰'
          ],
          relMore: 'å»¶ä¼¸é˜…è¯»ï¼š',
          backHome: 'â† è¿”å›é¦–é¡µ',
          backTop: 'å›åˆ°é¡¶éƒ¨ â†‘'
        }
      };

      function applyLang(lang){
        const t = T[lang];
        // é¡¶éƒ¨
        document.getElementById('homeLink').textContent = t.home;
        document.getElementById('pubdate').textContent = t.pubdate;
        document.getElementById('btnEN').className = 'px-2 py-1 rounded ' + (lang==='en' ? 'bg-black text-white':'hover:bg-neutral-100');
        document.getElementById('btnZH').className = 'px-2 py-1 rounded ' + (lang==='zh' ? 'bg-black text-white':'hover:bg-neutral-100');

        // æ ‡é¢˜/å‰¯æ ‡é¢˜/å›¾æ³¨
        document.getElementById('title').textContent = '';
        document.getElementById('title').innerHTML = t.title;
        document.getElementById('subtitle').innerHTML = t.subtitle;
        document.getElementById('fig1cap').innerHTML = t.fig1cap;

        // ç›®å½•
        document.getElementById('tocTitle').textContent = t.tocTitle;
        document.getElementById('toc1').textContent = t.toc1;
        document.getElementById('toc2').textContent = t.toc2;
        document.getElementById('toc3').textContent = t.toc3;
        document.getElementById('toc4').textContent = t.toc4;
        document.getElementById('toc5').textContent = t.toc5;

        // Motivation
        document.getElementById('motivationTitle').textContent = t.motivationTitle;
        document.getElementById('motivationBody').innerHTML = t.motivationBody;

        // NeurIPS'24
        document.getElementById('n24Title').textContent = t.n24Title;
        document.getElementById('n24Body').innerHTML = t.n24Body;
        const n24B = document.getElementById('n24Bullets');
        n24B.innerHTML = '';
        t.n24Bullets.forEach(li=>{ const el=document.createElement('li'); el.innerHTML=li; n24B.appendChild(el); });
        document.getElementById('fig2cap').innerHTML = t.fig2cap;
        document.getElementById('n24Paper').textContent = t.n24Paper;
        document.getElementById('n24Code').textContent = t.n24Code;
        document.getElementById('n24Proj').textContent = t.n24Proj;

        // NeurIPS'25
        document.getElementById('n25Title').textContent = t.n25Title;
        document.getElementById('n25Body').innerHTML = t.n25Body;
        const n25B = document.getElementById('n25Bullets');
        n25B.innerHTML = '';
        t.n25Bullets.forEach(li=>{ const el=document.createElement('li'); el.innerHTML=li; n25B.appendChild(el); });
        document.getElementById('n25Paper').textContent = t.n25Paper;

        // å¯¹æ¯”/æ¼”è¿›
        document.getElementById('cmpTitle').textContent = t.cmpTitle;
        document.getElementById('cmpL').textContent = t.cmpL;
        const cmpLB = document.getElementById('cmpLBullets');
        cmpLB.innerHTML = '';
        t.cmpLBullets.forEach(li=>{ const el=document.createElement('li'); el.innerHTML=li; cmpLB.appendChild(el); });
        document.getElementById('cmpR').textContent = t.cmpR;
        const cmpRB = document.getElementById('cmpRBullets');
        cmpRB.innerHTML = '';
        t.cmpRBullets.forEach(li=>{ const el=document.createElement('li'); el.innerHTML=li; cmpRB.appendChild(el); });

        // ç›¸å…³å·¥ä½œ
        document.getElementById('relTitle').textContent = t.relTitle;
        document.getElementById('relBody').innerHTML = t.relBody;
        const relList = document.getElementById('relList');
        relList.innerHTML = '';
        t.relList.forEach(li=>{ const el=document.createElement('li'); el.innerHTML=li; relList.appendChild(el); });
        document.getElementById('relMore').firstChild.textContent = t.relMore + ' ';
        // åº•éƒ¨æŒ‰é’®
        document.getElementById('backHome').textContent = t.backHome;
        document.getElementById('backTop').textContent = t.backTop;

        setLang(lang);
      }

      // åˆå§‹åŒ–è¯­è¨€
      const langInit = getLang();
      setLang(langInit);
      applyLang(langInit);

      // åˆ‡æ¢äº‹ä»¶
      document.getElementById('btnEN').addEventListener('click', ()=>applyLang('en'));
      document.getElementById('btnZH').addEventListener('click', ()=>applyLang('zh'));
    </script>
  </body>
</html>
